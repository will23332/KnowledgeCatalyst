# ============================================================================
# API Key Configuration
# ============================================================================
# Choose ONE of the following based on your setup:
# - OpenAI API key (for standard OpenAI models)
# - Azure OpenAI key (configured in LLM_MODEL_CONFIG section below)
# - Other providers (Gemini, Anthropic, etc.)
OPENAI_API_KEY=your-openai-api-key-here

# ============================================================================
# Embedding Configuration
# ============================================================================
# Options: "openai", "vertexai", or "all-MiniLM-L6-v2" (local model - recommended for getting started)
EMBEDDING_MODEL="all-MiniLM-L6-v2"
RAGAS_EMBEDDING_MODEL=""  # Leave blank to use all-MiniLM-L6-v2 for RAGAS embeddings
IS_EMBEDDING="TRUE"
ENTITY_EMBEDDING="TRUE"   # TRUE or FALSE - Enable entity embeddings for vector search

# ============================================================================
# Search Configuration
# ============================================================================
KNN_MIN_SCORE="0.94"
EFFECTIVE_SEARCH_RATIO=5
DUPLICATE_SCORE_VALUE=0.97
DUPLICATE_TEXT_DISTANCE=3

# ============================================================================
# Processing Configuration
# ============================================================================
NUMBER_OF_CHUNKS_TO_COMBINE=6
UPDATE_GRAPH_CHUNKS_PROCESSED=20
MAX_TOKEN_CHUNK_SIZE=50000  # Max token size for processing/extracting file content (increased from 2000 for better document coverage)

# ============================================================================
# Neo4j Database Configuration
# ============================================================================
# OPTION 1: Using Docker Compose with included Neo4j container (recommended for local development)
NEO4J_URI="bolt://neo4j:7687"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="your-secure-password-here"  # IMPORTANT: Change this to a strong password!
NEO4J_DATABASE="neo4j"

# OPTION 2: Using external Neo4j instance (Neo4j Aura or self-hosted)
# Uncomment and configure these lines if using external Neo4j:
# NEO4J_URI="neo4j+s://your-instance.databases.neo4j.io:7687"  # For Neo4j Aura
# NEO4J_URI="bolt://your-neo4j-host:7687"  # For self-hosted Neo4j
# NEO4J_USERNAME="neo4j"
# NEO4J_PASSWORD="your-neo4j-password-here"
# NEO4J_DATABASE="neo4j"

NEO4J_USER_AGENT=""
ENABLE_USER_AGENT=""

# Backend Configuration
BACKEND_URL="http://backend:8000"

# Cloud Provider Configuration
# Google Cloud Platform
GEMINI_ENABLED=False  # Options: True or False
GCP_LOG_METRICS_ENABLED=False  # Options: True or False
GCS_FILE_CACHE=""  # Save files to GCS or local - Options: True or False

# AWS Configuration
AWS_ACCESS_KEY_ID=""
AWS_SECRET_ACCESS_KEY=""

# LangChain Configuration (Optional)
LANGCHAIN_API_KEY=""
LANGCHAIN_PROJECT=""
LANGCHAIN_TRACING_V2=""
LANGCHAIN_ENDPOINT=""

# ============================================================================
# LLM Model Configuration
# ============================================================================
# Default model for chat and graph processing
DEFAULT_DIFFBOT_CHAT_MODEL="azure_ai_gpt_4o_mini"  # Change to your preferred model
GRAPH_CLEANUP_MODEL="azure_ai_gpt_4o_mini"  # Model used for graph cleanup operations

# ----------------------------------------------------------------------------
# OpenAI Models (Standard API)
# ----------------------------------------------------------------------------
LLM_MODEL_CONFIG_openai_gpt_3.5="gpt-3.5-turbo-0125,your-openai-api-key"
LLM_MODEL_CONFIG_openai_gpt_4o_mini="gpt-4o-mini-2024-07-18,your-openai-api-key"
LLM_MODEL_CONFIG_openai_gpt_4o="gpt-4o-2024-11-20,your-openai-api-key"
LLM_MODEL_CONFIG_openai-gpt-o3-mini="o3-mini-2025-01-31,your-openai-api-key"

# ----------------------------------------------------------------------------
# Azure OpenAI Models (Recommended for Enterprise)
# ----------------------------------------------------------------------------
# Format: deployment_name,endpoint,api_key,api_version
# Example with gpt-4o-mini (cost-effective):
LLM_MODEL_CONFIG_azure_ai_gpt_4o_mini="gpt-4o-mini,https://YOUR-RESOURCE.openai.azure.com/,your-azure-api-key,2024-08-01-preview"
# Example with gpt-4o:
LLM_MODEL_CONFIG_azure_ai_gpt_4o="gpt-4o,https://YOUR-RESOURCE.openai.azure.com/,your-azure-api-key,2024-08-01-preview"
# Example with gpt-3.5-turbo:
LLM_MODEL_CONFIG_azure_ai_gpt_35="gpt-35-turbo,https://YOUR-RESOURCE.openai.azure.com/,your-azure-api-key,2024-08-01-preview"

# ----------------------------------------------------------------------------
# Google Gemini Models
# ----------------------------------------------------------------------------
LLM_MODEL_CONFIG_gemini_1.5_pro="gemini-1.5-pro-002"
LLM_MODEL_CONFIG_gemini_1.5_flash="gemini-1.5-flash-002"
LLM_MODEL_CONFIG_gemini_2.0_flash="gemini-2.0-flash-001"

# ----------------------------------------------------------------------------
# Anthropic Claude Models
# ----------------------------------------------------------------------------
LLM_MODEL_CONFIG_anthropic_claude_3_5_sonnet="claude-3-5-sonnet-20241022,your-anthropic-api-key"

# ----------------------------------------------------------------------------
# AWS Bedrock Models
# ----------------------------------------------------------------------------
LLM_MODEL_CONFIG_bedrock_claude_3_5_sonnet="anthropic.claude-3-5-sonnet-20241022-v2:0,aws_access_key_id,aws_secret_access_key,us-east-1"
LLM_MODEL_CONFIG_bedrock_nova_micro_v1="amazon.nova-micro-v1:0,aws_access_key,aws_secret_key,us-east-1"
LLM_MODEL_CONFIG_bedrock_nova_lite_v1="amazon.nova-lite-v1:0,aws_access_key,aws_secret_key,us-east-1"
LLM_MODEL_CONFIG_bedrock_nova_pro_v1="amazon.nova-pro-v1:0,aws_access_key,aws_secret_key,us-east-1"
BEDROCK_EMBEDDING_MODEL="amazon.titan-embed-text-v1,aws_access_key,aws_secret_key,us-east-1"

# ----------------------------------------------------------------------------
# Ollama (Local Models - No API Key Required!)
# ----------------------------------------------------------------------------
# Run Ollama locally for complete privacy and no API costs
# Install: https://ollama.ai or use the Docker service below
# Popular models: llama3, llama3.2, mistral, phi3, qwen2.5, deepseek-r1

# Example configurations (uncomment and use the model you have installed):
LLM_MODEL_CONFIG_ollama_llama3="llama3,http://localhost:11434"
LLM_MODEL_CONFIG_ollama_llama3_2="llama3.2,http://localhost:11434"
LLM_MODEL_CONFIG_ollama_mistral="mistral,http://localhost:11434"
LLM_MODEL_CONFIG_ollama_phi3="phi3,http://localhost:11434"
LLM_MODEL_CONFIG_ollama_qwen2_5="qwen2.5,http://localhost:11434"
LLM_MODEL_CONFIG_ollama_deepseek_r1="deepseek-r1:8b,http://localhost:11434"

# If using Ollama in Docker Compose, use: http://ollama:11434
# LLM_MODEL_CONFIG_ollama_llama3="llama3,http://ollama:11434"

# ----------------------------------------------------------------------------
# Other LLM Providers
# ----------------------------------------------------------------------------
LLM_MODEL_CONFIG_groq_llama3_70b="llama3-70b-8192,https://api.groq.com/openai/v1,your-groq-api-key"
LLM_MODEL_CONFIG_fireworks_llama_v3_70b="accounts/fireworks/models/llama-v3-70b-instruct,your-fireworks-api-key"
LLM_MODEL_CONFIG_fireworks_deepseek_r1="accounts/fireworks/models/deepseek-r1,your-fireworks-api-key"
LLM_MODEL_CONFIG_fireworks_deepseek_v3="accounts/fireworks/models/deepseek-v3,your-fireworks-api-key"
LLM_MODEL_CONFIG_diffbot="diffbot,your-diffbot-api-key"

# ============================================================================
# Optional Configurations
# ============================================================================
# YouTube transcript proxy (if needed for accessing YouTube transcripts)
YOUTUBE_TRANSCRIPT_PROXY=""  # Format: https://user:pass@domain:port

LLM_MODEL_CONFIG_model_version=""
